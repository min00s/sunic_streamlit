# --- 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ---
# pip install streamlit python-dotenv langchain-openai "pinecone-client>=3.0.0" pandas openpyxl tabulate gspread google-auth-oauthlib python-docx "python-pptx" PyMuPDF chardet beautifulsoup4 lxml

import streamlit as st
import os
import io
import json
import requests
import zipfile
import fitz  # PyMuPDF
import pandas as pd
import chardet
import xml.etree.ElementTree as ET
import urllib.parse
import base64
import re
import time
from docx import Document
from pptx import Presentation
from bs4 import BeautifulSoup
from typing import Optional
from dotenv import load_dotenv, find_dotenv
from pathlib import Path


# LangChain ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.prompts import PromptTemplate
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser

# Pinecone & Google Sheets ë¼ì´ë¸ŒëŸ¬ë¦¬
import pinecone
import gspread
from google.oauth2.service_account import Credentials

# --- 2. í™˜ê²½ë³€ìˆ˜ ë° ê¸°ë³¸ ì„¤ì • ---
load_dotenv(find_dotenv())
OPENAI_API_KEY: Optional[str] = os.getenv("OPENAI_API_KEY")
GH_TOKEN: Optional[str] = os.getenv("GH_TOKEN")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_INDEX = os.getenv("PINECONE_INDEX", "pr-index")
GOOGLE_SHEET_KEY = os.getenv("GOOGLE_SHEET_KEY")
SERVICE_ACCOUNT_FILE = "credentials.json"
FEW_SHOT_SHEET_NAME = os.getenv("FEW_SHOT_SHEET_NAME", "Few-Shot Examples")

# --- í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(page_title="S-Kape", layout="wide")

# --- ê³µí†µ ìŠ¤íƒ€ì¼ ---
st.markdown("""
<style>
/* ì»¨í…Œì´ë„ˆ í­ ë°°ë„ˆ */
.section-band {
    width:100%;
    background:#F8D7DA;
    border-radius:10px;
    padding:10px 14px;
    margin:20px 0 12px;
}
.section-band .title {
    display:block;
    margin:0;
    font-family: inherit;
    color: inherit;
    font-weight:700;
    line-height:1.4;
    text-align:center;
    font-size:1.15rem;
}
/* í—¤ë” */
.header-wrap {
    display:flex; align-items:center; gap:16px;
    padding:8px 0 16px; margin:0;
    border-bottom:1px solid rgba(0,0,0,.06);
}
.brand-icon { width:300px; height:auto; object-fit:contain; display:block; }
.brand-wordmark { height:150px; width:auto; display:block; }
.tagline { margin:.25rem 0 0 0; opacity:.8; font-size:1rem; }
@media (max-width: 768px) {
    .brand-icon{ width:110px; }
    .brand-wordmark{ height:40px; }
}
/* íƒ­ ìŠ¤íƒ€ì¼ */
.stTabs [data-baseweb="tab-list"] {
    background-color: #FF6C6C;
    border-radius: 8px;
    padding: 0;
    display: flex;
    justify-content: space-between;
}
.stTabs [data-baseweb="tab"] {
    flex: 1;
    text-align: center;
    color: white;
    padding: 8px 0;
    border-right: 1px solid rgba(255,255,255,0.4);
}
.stTabs [data-baseweb="tab"]:last-child { border-right: none; }
.stTabs [aria-selected="true"] {
    font-weight: 700;
    background-color: rgba(0,0,0,0.1);
}
/* Footer */
.footer {
    width: 100%;
    background-color: #8C8C8C;
    color: #fff;
    text-align: center;
    padding: 12px 0;
    font-size: 14px;
    border-top: 1px solid rgba(255,255,255,0.2);
    margin-top: 30px;
}
</style>
""", unsafe_allow_html=True)


# --- 3. ê³µí†µ LLM, RAG ë° í—¬í¼ í•¨ìˆ˜ ì´ˆê¸°í™” ---

# ìš”êµ¬ì‚¬í•­ ì¶”ì¶œìš© LLM
req_llm = ChatOpenAI(model="gpt-4o", api_key=OPENAI_API_KEY, temperature=0.1)
req_output_parser = JsonOutputParser()

# Side-Effect ì˜ˆì¸¡ìš© LLM (RAG)
se_llm = ChatOpenAI(model="gpt-4o", temperature=0.2, api_key=OPENAI_API_KEY, max_tokens=2000)

# Pinecone ì´ˆê¸°í™”
pinecone_index = None
emb = None
if PINECONE_API_KEY:
    try:
        pc = pinecone.Pinecone(api_key=PINECONE_API_KEY)
        if PINECONE_INDEX not in pc.list_indexes().names():
            st.info(f"Pinecone ì¸ë±ìŠ¤ '{PINECONE_INDEX}'ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ëª‡ ë¶„ ì •ë„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            pc.create_index(
                name=PINECONE_INDEX,
                dimension=1536,
                metric="cosine",
                spec=pinecone.ServerlessSpec(cloud="aws", region="us-east-1")
            )
            while not pc.describe_index(PINECONE_INDEX).status["ready"]:
                time.sleep(2)
        pinecone_index = pc.Index(PINECONE_INDEX)
        emb = OpenAIEmbeddings(api_key=OPENAI_API_KEY)
    except Exception as e:
        st.error(f"Pinecone ì´ˆê¸°í™” ì‹¤íŒ¨: {e}. .env íŒŒì¼ì˜ PINECONE_API_KEYë¥¼ í™•ì¸í•˜ì„¸ìš”.")
else:
    st.warning("PINECONE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ Side-Effect ì˜ˆì¸¡ ì‹œ ìœ ì‚¬ì‚¬ë¡€ ê²€ìƒ‰(RAG) ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")


# RAG í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (JSON ì¶œë ¥ìš©)
RAG_PROMPT_TEMPLATE = PromptTemplate.from_template("""
ë‹¹ì‹ ì€ 10ë…„ ê²½ë ¥ì˜ ì‹œë‹ˆì–´ ë°±ì—”ë“œ ê°œë°œìì´ì QA ìë™í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì œê³µëœ ì†Œí”„íŠ¸ì›¨ì–´ ìš”êµ¬ì‚¬í•­, ì½”ë“œ ë³€ê²½ ë‚´ì—­, ê·¸ë¦¬ê³  ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€ë¥¼ ì‹ ì¤‘í•˜ê²Œ ê²€í† í•˜ì—¬ ë°œìƒí•  ìˆ˜ ìˆëŠ” ì ì¬ì  ì‚¬ì´ë“œ ì´í™íŠ¸ë¥¼ ì˜ˆì¸¡í•˜ì„¸ìš”.
ì½”ë“œ ë³€ê²½(Î”)ì— ê·¼ê±°í•˜ì—¬ ì‚¬ì´ë“œ ì´í™íŠ¸ ìœ í˜•ì„ ë¶„ë¥˜í•˜ê³ , ì˜í–¥ ì˜ì—­ì„ ì„¤ëª…í•˜ë©°, ê²€ì¦ ê°€ëŠ¥í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ ìƒì„±í•˜ì„¸ìš”.
ë‹¹ì‹ ì˜ ì¶œë ¥ì€ **ë°˜ë“œì‹œ ì•„ë˜ ìŠ¤í‚¤ë§ˆë¥¼ ì—„ê²©íˆ ë”°ë¥´ëŠ” ë‹¨ì¼ JSON ê°ì²´**ì—¬ì•¼ í•©ë‹ˆë‹¤. JSON ê°ì²´ ì™¸ë¶€ì— ë‹¤ë¥¸ í…ìŠ¤íŠ¸ë‚˜ ì„¤ëª…ì„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

[ì°¸ê³ : ê³¼ê±° ìœ ì‚¬ PR ë° ì˜ˆì¸¡ ê²°ê³¼]
{rag_context}

[ì…ë ¥]
ìš”êµ¬ì‚¬í•­:
{requirements}

ì½”ë“œ ë³€ê²½ ë‚´ì—­:
{code_diff}

[JSON ì¶œë ¥ ìŠ¤í‚¤ë§ˆ]
{{
  "summary": "í•µì‹¬ ì½”ë“œ ë³€ê²½ì— ëŒ€í•œ 1~2 ë¬¸ì¥ ìš”ì•½",
  "candidates": [
    {{
      "rank": "INTEGER (1, 2, ë˜ëŠ” 3)",
      "type": "STRING (ì„ íƒ: ê¸°ëŠ¥ íšŒê·€, ì˜ˆì™¸ ì²˜ë¦¬ ëˆ„ë½, ìƒíƒœ ë¶ˆì¼ì¹˜, ì„±ëŠ¥ ì €í•˜, ê¸°íƒ€)",
      "confidence": "FLOAT (0.0 ë¶€í„° 1.0)",
      "reason": "STRING (ì˜ˆì¸¡ ê·¼ê±°ë¥¼ ì„¤ëª…í•˜ëŠ” í•œ ë¬¸ì¥)"
    }}
  ],
  "explanation": [
    "STRING (í˜•ì‹: ì˜í–¥ ì˜ì—­ - ì„¤ëª…; ì´ìœ )"
  ],
  "test_cases": [
    {{
      "TC-ID": "STRING (ì˜ˆ: TC-001)",
      "related_type": "STRING ('candidates'ì˜ 'type'ê³¼ ì¼ì¹˜)",
      "target_area": "STRING (í…ŒìŠ¤íŠ¸ ëŒ€ìƒ ì˜ì—­)",
      "purpose": "STRING (í…ŒìŠ¤íŠ¸ ëª©ì )",
      "procedure": "STRING (í•œê¸€ë¡œ ì‘ì„±ëœ í•œ ë¬¸ì¥ì˜ í…ŒìŠ¤íŠ¸ ì ˆì°¨)",
      "expected_result": "STRING (ì˜ˆìƒ ê²°ê³¼)"
    }}
  ]
}}

**[ì¤‘ìš” ê·œì¹™]
- 'explanation' ë°°ì—´ì˜ í•­ëª© ê°œìˆ˜ëŠ” 'candidates' ë°°ì—´ì˜ í•­ëª© ê°œìˆ˜ì™€ ë°˜ë“œì‹œ ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤. 'candidates'ì—ì„œ ì œì‹œëœ ëª¨ë“  ìœ í˜•ì— ëŒ€í•œ ì„¤ëª…ì„ ë¹ ì§ì—†ì´ ì‘ì„±í•˜ì„¸ìš”.**
""")

# --- 4. í—¬í¼ í•¨ìˆ˜ ì •ì˜ ---
def spacer(px: int = 25):
    st.markdown(f"<div style='height:{px}px'></div>", unsafe_allow_html=True)

def banner(
    filename: str,
    *,
    width: Optional[int] = None,
    max_width: Optional[int] = None,
    max_height: Optional[int] = None,
    width_ratio: Optional[float] = None,
    caption: Optional[str] = None
) -> None:
    p = (Path(__file__).parent / filename) if "__file__" in globals() else Path(filename)
    if not p.exists():
        st.warning(f"ë°°ë„ˆ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤: {p}")
        return
    data = p.read_bytes()
    b64 = base64.b64encode(data).decode()

    def show_css_image(max_w: Optional[int], max_h: Optional[int], use_container: bool = False):
        styles = []
        if use_container or max_w is not None: styles.append("width:100%")
        if max_w is not None: styles.append(f"max-width:{max_w}px")
        if max_h is not None:
            styles.extend([f"max-height:{max_h}px", "height:auto", "object-fit:contain"])
        style_str = "; ".join(styles)
        st.markdown(
            f"<img src='data:image/png;base64,{b64}' style='{style_str}; display:block; margin:0 auto;'>"
            + (f"<div style='text-align:center; opacity:.7; font-size:.9rem'>{caption}</div>" if caption else ""),
            unsafe_allow_html=True
        )

    if width_ratio is not None and 0 < width_ratio <= 1:
        side = (1 - width_ratio) / 2
        c1, c2, c3 = st.columns([side, width_ratio, side])
        with c2:
            if max_height is not None or max_width is not None:
                show_css_image(max_w=max_width, max_h=max_height, use_container=True)
            else:
                st.image(data, width=width, use_container_width=(width is None), caption=caption)
        return

    if width is not None:
        st.image(data, width=width, caption=caption)
    elif max_height is not None or max_width is not None:
        show_css_image(max_w=max_width, max_h=max_height)
    else:
        st.image(data, use_container_width=True, caption=caption)

def section_heading(text: str, bg: str = "#F8D7DA", color: str = "inherit"):
    st.markdown(
        f"""<div class="section-band" style="background:{bg};">
               <span class="title" style="color:{color};">{text}</span>
           </div>""",
        unsafe_allow_html=True
    )

def extract_text_from_uploaded_file(uploaded_file):
    file_name = uploaded_file.name
    ext = os.path.splitext(file_name)[1].lower()
    file_bytes = uploaded_file.getvalue()
    try:
        if ext in [".txt", ".md"]:
            encoding = chardet.detect(file_bytes)["encoding"] or "utf-8"
            return file_bytes.decode(encoding)
        elif ext == ".docx":
            return "\n".join([p.text for p in Document(io.BytesIO(file_bytes)).paragraphs])
        elif ext == ".pptx":
            prs = Presentation(io.BytesIO(file_bytes))
            return "\n".join(shape.text for slide in prs.slides for shape in slide.shapes if hasattr(shape, "text"))
        elif ext == ".pdf":
            with fitz.open(stream=file_bytes, filetype="pdf") as pdf:
                return "\n".join([page.get_text() for page in pdf])
        elif ext in [".csv", ".xlsx", ".cell"]:
            df = pd.read_excel(io.BytesIO(file_bytes)) if ext in [".xlsx", ".cell"] else pd.read_csv(io.BytesIO(file_bytes))
            return df.to_string(index=False)
        elif ext == ".json":
            return json.dumps(json.load(io.StringIO(file_bytes.decode('utf-8'))), indent=2, ensure_ascii=False)
        elif ext == ".xml":
            return ET.tostring(ET.fromstring(file_bytes), encoding="unicode")
        elif ext == ".hwpx":
            with open(file_name, "wb") as f: f.write(file_bytes)
            with zipfile.ZipFile(file_name, 'r') as zipf:
                section_files = [f for f in zipf.namelist() if f.startswith("Contents/section") and f.endswith(".xml")]
                text_chunks = []
                for section in section_files:
                    with zipf.open(section) as file:
                        soup = BeautifulSoup(file.read(), "xml")
                        texts = soup.find_all("TEXT")
                        text_chunks.extend(t.text for t in texts if t.text)
            os.remove(file_name)
            return "\n".join(text_chunks)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {ext}")
    except Exception as e:
        raise ValueError(f"íŒŒì¼ '{file_name}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def read_excel_to_string(file):
    try:
        return pd.read_excel(file, engine='openpyxl').to_markdown(index=False)
    except Exception as e:
        st.error(f"ì—‘ì…€ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
        return None

def make_code_diff(orig: str, mod: str) -> str:
    return f"""[ê¸°ì¡´ ì½”ë“œ]\n{orig or 'N/A'}\n\n[ìˆ˜ì •ëœ ì½”ë“œ]\n{mod or 'N/A'}"""

def parse_pr_url(pr_url: str):
    m = re.match(r"https://github.com/(?P<owner>[\w.-]+)/(?P<repo>[\w.-]+)/pull/(?P<num>\d+)", pr_url)
    if not m:
        raise ValueError("PR URL í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. (ì˜ˆ: https://github.com/owner/repo/pull/123)")
    return m.group("owner"), m.group("repo"), int(m.group("num"))

def _gh_get(url: str, raw: bool = False, **extra_hdr):
    headers = {"Accept": "application/vnd.github+json", **({"Authorization": f"Bearer {GH_TOKEN}"} if GH_TOKEN else {}), **extra_hdr}
    resp = requests.get(url, headers=headers, timeout=30)
    resp.raise_for_status()
    return resp.text if raw else resp.json()

def fetch_pr_code_only(owner: str, repo: str, pr_number: int):
    API_ROOT = "https://api.github.com"
    pr = _gh_get(f"{API_ROOT}/repos/{owner}/{repo}/pulls/{pr_number}")
    base_sha, head_sha = pr["base"]["sha"], pr["head"]["sha"]

    def get_file_at_sha(path: str, sha: str):
        try:
            meta = _gh_get(f"{API_ROOT}/repos/{owner}/{repo}/contents/{path}?ref={sha}")
            return base64.b64decode(meta["content"]).decode('utf-8', errors='ignore')
        except Exception:
            return f"# '{path}' ê²½ë¡œì˜ íŒŒì¼ì„ '{sha}' ì»¤ë°‹ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    files = _gh_get(f"{API_ROOT}/repos/{owner}/{repo}/pulls/{pr_number}/files")
    py_files = [f for f in files if f["filename"].endswith(".py")]
    if not py_files and not files:
        raise ValueError("PRì— ë³€ê²½ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    target_file = py_files[0] if py_files else files[0]
    if not py_files:
        st.warning(f"PRì—ì„œ íŒŒì´ì¬(.py) íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì²« ë²ˆì§¸ íŒŒì¼ '{target_file['filename']}'ì„ ëŒ€ì‹  ì‚¬ìš©í•©ë‹ˆë‹¤.")

    original_code = get_file_at_sha(target_file['filename'], base_sha)
    modified_code = get_file_at_sha(target_file['filename'], head_sha)
    return original_code, modified_code

def generate_modified_code(requirements: str, original_code: str) -> str:
    code_gen_llm = ChatOpenAI(model="gpt-4o", temperature=0.1, api_key=OPENAI_API_KEY)
    prompt = f"""
    ë‹¹ì‹ ì€ 10ë…„ì°¨ ì‹œë‹ˆì–´ ê°œë°œìì…ë‹ˆë‹¤. ì•„ë˜ [ìš”êµ¬ì‚¬í•­]ì„ ë°˜ì˜í•˜ì—¬ [ì›ë³¸ ì½”ë“œ]ë¥¼ ìˆ˜ì •í•˜ê³ , ìˆ˜ì •ëœ ì „ì²´ ì½”ë“œë§Œ ì‘ë‹µìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”.
    ì¶”ê°€ë¡œ ì–´ë””ê°€ ìˆ˜ì •ì´ ë˜ì—ˆëŠ”ì§€ ìˆ˜ì •ëœ ë¶€ë¶„ì— ì£¼ì„ë„ ë‹¬ì•„ì£¼ì„¸ìš”.

    [ìš”êµ¬ì‚¬í•­]
    {requirements}

    [ì›ë³¸ ì½”ë“œ]
    ```python
    {original_code}
    ```
    """
    response = code_gen_llm.invoke([HumanMessage(content=prompt)])
    code_content = response.content.strip()
    
    match = re.search(r"```python\n(.*?)\n```", code_content, re.DOTALL)
    if match:
        return match.group(1).strip()
    if code_content.startswith("```"):
        return '\n'.join(code_content.split('\n')[1:-1])
    return code_content

def embed_text(text: str):
    return emb.embed_query(text) if emb else None

def upsert_to_pinecone(pr_id: str, text: str, meta: dict):
    if pinecone_index and (vec := embed_text(text)):
        pinecone_index.upsert(vectors=[(pr_id, vec, meta)])
        st.sidebar.info(f"PR '{pr_id}' ì •ë³´ê°€ Pinecone DBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def search_similar_prs(query_text: str, top_k=3):
    if pinecone_index and (qvec := embed_text(query_text)):
        return pinecone_index.query(vector=qvec, top_k=top_k, include_metadata=True).matches or []
    return []
def get_github_file(owner, repo, file_path, branch="main"):
    url = f"https://api.github.com/repos/{owner}/{repo}/contents/{file_path}?ref={branch}"
    headers = {"Authorization": f"Bearer {GH_TOKEN}"}
    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        raise Exception(f"GitHub API ì˜¤ë¥˜: {response.status_code}\n{response.text}")
    data = response.json()
    if "content" in data:
        file_content = base64.b64decode(data["content"]).decode("utf-8")
        return file_content
    else:
        raise Exception(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")


def render_rag_results(result_data):
    """RAG ê²°ê³¼ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë°›ì•„ í™”ë©´ì— ìŠ¤íƒ€ì¼ì„ ì ìš©í•˜ì—¬ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜"""
    st.markdown("""
    <style>
    .section-divider {
        padding: 4px 0px 4px 10px; margin-top: 20px; margin-bottom: 12px;
        border-left: 5px solid #F8D7DA; border-top-left-radius: 5px; border-bottom-left-radius: 5px;
        font-weight: bold; font-size: 1.2em; color: #333;
    }
    </style>""", unsafe_allow_html=True)
    
    st.markdown('<div class="section-divider">Î” ìš”ì•½</div>', unsafe_allow_html=True)
    if result_data.get("summary"):
        st.write(f"ğŸ”¹ {result_data.get('summary')}")

    st.markdown('<div class="section-divider">ìœ í˜• í›„ë³´ (ìµœëŒ€ 3ê°œ, ì¤‘ìš”ë„ ìˆœ)</div>', unsafe_allow_html=True)
    
    with st.expander("â„¹ï¸ ì‹ ë¢°ë„ë€?"):
        st.markdown("""
        ì´ ì ìˆ˜ëŠ” ìš”êµ¬ì‚¬í•­, ì½”ë“œ ë³€ê²½ ë‚´ì—­, ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€ë¥¼ ì¢…í•©í•˜ì—¬ AIê°€ ë‚´ë¦° íŒë‹¨ì˜ í™•ì‹ ë„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n 
        ì‹ ë¢°ë„ê°€ ë†’ì€ í•­ëª©ì„ ìµœìš°ì„ ìœ¼ë¡œ ê²€í† í•˜ë©´ íš¨ìœ¨ì ìœ¼ë¡œ ì ì¬ì  ìœ„í—˜ì„ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n
        
        ì‹ ë¢°ë„ ë†’ìŒ (0.9 ì´ìƒ): ë°˜ë“œì‹œ ê°€ì¥ ë¨¼ì € í™•ì¸í•´ì•¼ í•  'ìœ„í—˜ ì‹ í˜¸'ì…ë‹ˆë‹¤. ì‹¤ì œ ë¬¸ì œì¼ ê°€ëŠ¥ì„±ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤.\n
        ì‹ ë¢°ë„ ì¤‘ê°„ (0.7 ~ 0.89): ì¶©ë¶„íˆ ë°œìƒ ê°€ëŠ¥í•œ ë¬¸ì œì´ë¯€ë¡œ ê¼¼ê¼¼íˆ ê²€í† í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤.\n
        ì‹ ë¢°ë„ ë‚®ìŒ (0.7 ë¯¸ë§Œ): ë°œìƒ í™•ë¥ ì€ ë‚®ì§€ë§Œ, ë†“ì¹  ìˆ˜ ìˆëŠ” ë¶€ë¶„ì„ ì§šì–´ì£¼ëŠ” 'ì°¸ê³  ì˜ê²¬'ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n
        
        """)

    if result_data.get("candidates"):
        try:
            df_candidates = pd.DataFrame(result_data.get("candidates"))
            df_candidates.columns = ["ìˆœìœ„", "ìœ í˜•", "ì‹ ë¢°ë„", "ê·¼ê±°"]
            st.table(df_candidates.set_index("ìˆœìœ„"))
        except Exception as e:
            st.error(f"ìœ í˜• í›„ë³´ í…Œì´ë¸” ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            st.json(result_data.get("candidates"))

    st.markdown('<div class="section-divider">ì„¤ëª…</div>', unsafe_allow_html=True)
    if result_data.get("explanation"):
        for item in result_data.get("explanation"):
            st.markdown(f"<li>{item}</li>", unsafe_allow_html=True)

    st.markdown('<div class="section-divider">í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤</div>', unsafe_allow_html=True)
    if result_data.get("test_cases"):
        try:
            df_test_cases = pd.DataFrame(result_data.get("test_cases"))
            df_test_cases.columns = ["TC-ID", "ê´€ë ¨ ìœ í˜•", "ëŒ€ìƒ ì˜ì—­", "í…ŒìŠ¤íŠ¸ ëª©ì ", "ì ˆì°¨(í•œê¸€ 1ë¬¸ì¥)", "ì˜ˆìƒ ê²°ê³¼"]
            st.table(df_test_cases.set_index("TC-ID"))
        except Exception as e:
            st.error(f"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ í…Œì´ë¸” ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            st.json(result_data.get("test_cases"))

# --- 5. UI ë Œë”ë§ ---
# ë¡œê³  ë° í—¤ë”
try:
    icon_b64 = base64.b64encode((Path(__file__).parent / "S-Kape_Logo_1.png").read_bytes()).decode()
    wordmark_b64 = base64.b64encode((Path(__file__).parent / "S-Kape_Logo_2.png").read_bytes()).decode()
    st.markdown(f"""
    <div class="header-wrap">
        <img class="brand-icon" src="data:image/png;base64,{icon_b64}" alt="S-Kape mascot">
        <div>
            <img class="brand-wordmark" src="data:image/png;base64,{wordmark_b64}" alt="S-Kape">
            <p class="tagline">íšŒì˜ë¡ ë¶„ì„ë¶€í„° ì½”ë“œ ë³€ê²½ì— ë”°ë¥¸ Side-Effect ì˜ˆì¸¡ê¹Œì§€, S-Kapeë¡œ ë²„ê·¸ ì§€ì˜¥ì—ì„œ íƒˆì¶œí•˜ì!</p>
        </div>
    </div>""", unsafe_allow_html=True)
except FileNotFoundError:
    st.title("S-Kape")
    st.subheader("íšŒì˜ë¡ ë¶„ì„ë¶€í„° ì½”ë“œ ë³€ê²½ì— ë”°ë¥¸ Side-Effect ì˜ˆì¸¡ê¹Œì§€, S-Kapeë¡œ ë²„ê·¸ ì§€ì˜¥ì—ì„œ íƒˆì¶œí•˜ì!")


# íƒ­ ìƒì„±
tab1, tab2, tab3 = st.tabs(["ìš”êµ¬ì‚¬í•­ ëª…ì„¸ì„œ ì¶”ì¶œ", "Side Effect ì˜ˆì¸¡", "Side Effect ì˜ˆì¸¡ Plus +"])

# ==================================================================================
# << TAB 1: ìš”êµ¬ì‚¬í•­ ëª…ì„¸ì„œ ì¶”ì¶œê¸° >>
# ==================================================================================
with tab1:
    banner("tab1.png", max_height=400)
    spacer()

    @st.cache_data(ttl=600)
    def load_req_few_shot_examples():
        if not GOOGLE_SHEET_KEY or not os.path.exists(SERVICE_ACCOUNT_FILE):
            st.sidebar.success("Google Sheets ì—°ë™ ì„±ê³µ!.")
            return []
        try:
            scope = ['[https://spreadsheets.google.com/feeds](https://spreadsheets.google.com/feeds)', '[https://www.googleapis.com/auth/drive](https://www.googleapis.com/auth/drive)']
            creds = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=scope)
            gc = gspread.authorize(creds)
            worksheet = gc.open_by_key(GOOGLE_SHEET_KEY).worksheet(FEW_SHOT_SHEET_NAME)
            all_records = worksheet.get_all_records()
            examples = []
            for record in all_records:
                user_input = record.pop('íšŒì˜ë¡', '')
                if 'í™•ì¸ì‚¬í•­' in record and isinstance(record['í™•ì¸ì‚¬í•­'], str):
                    record['í™•ì¸ì‚¬í•­'] = [item.strip() for item in record['í™•ì¸ì‚¬í•­'].split('\n') if item.strip()]
                if user_input:
                    examples.append(HumanMessage(content=f"íšŒì˜ë¡:\n{user_input}"))
                    examples.append(AIMessage(content=json.dumps([record], ensure_ascii=False)))
            st.sidebar.success("Google Sheets Few-shot ì˜ˆì‹œ ë¡œë“œ ì„±ê³µ!")
            return examples
        except Exception as e:
            st.sidebar.success(f"Google Sheets ë¡œë“œ ì„±ê³µ!")
            return []

    def extract_requirements_from_text(text, few_shot_examples):
        system_prompt = """
        You are a requirements engineer that always outputs only a valid JSON array.
        Analyze the provided meeting minutes and extract all requirements.
        Follow the provided JSON schema. Do not include any explanations or text outside the JSON array.
        JSON_SCHEMA: [
            {{
                "No.": "INTEGER", "ìš”êµ¬ì‚¬í•­ ID" : "STRING (REQ-001...)", "íŒŒì¼ëª…": "STRING", "êµ¬ë¶„": "STRING",
                "ë¶„ë¥˜": "STRING", "ìœ í˜•": "STRING", "ê¸°ëŠ¥ ë¶„ë¥˜ 1": "STRING", "ê¸°ëŠ¥ ë¶„ë¥˜ 2": "STRING",
                "ìš”êµ¬ì‚¬í•­ ëª…": "STRING", "ìš”êµ¬ì‚¬í•­ ìƒì„¸ ë‚´ìš©": "STRING", "í™•ì¸ì‚¬í•­": "ARRAY of STRINGS"
            }}
        ]
        """
        prompt = ChatPromptTemplate.from_messages(
            [("system", system_prompt)] + few_shot_examples + [("human", "íšŒì˜ë¡:\n{text}")]
        )
        chain = prompt | req_llm | req_output_parser
        return chain.invoke({"text": text[:15000]})

    uploaded_files = st.file_uploader(
        "íšŒì˜ë¡ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš” (txt, docx, pdf, pptx, csv, xlsx, json, xml, hwpx)",
        type=["txt", "docx", "pdf", "pptx", "xlsx", "csv", "json", "xml", "hwpx"],
        accept_multiple_files=True,
        key="minutes_uploader_tab1"
    )
    
    if uploaded_files: st.write(f"**ì„ íƒëœ íŒŒì¼: {len(uploaded_files)}ê°œ**")

    if st.button("ëª…ì„¸ì„œ ìƒì„±í•˜ê¸°", use_container_width=True, key="generate_req_btn_tab1"):
        if not uploaded_files:
            st.error("âš ï¸ íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
        else:
            with st.spinner("íŒŒì¼ ë¶„ì„ ë° ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ ì¤‘..."):
                try:
                    combined_text = ""
                    for file in uploaded_files:
                        text = extract_text_from_uploaded_file(file)
                        combined_text += f"\n\n--- ë¬¸ì„œ ì‹œì‘: [{file.name}] ---\n{text.strip()}\n--- ë¬¸ì„œ ë: [{file.name}] ---\n"
                    
                    few_shot_examples = load_req_few_shot_examples()
                    requirements_data = extract_requirements_from_text(combined_text, few_shot_examples)
                    
                    if requirements_data:
                        df = pd.DataFrame(requirements_data)
                        output_buffer = io.BytesIO()
                        with pd.ExcelWriter(output_buffer, engine='openpyxl') as writer:
                            df.to_excel(writer, index=False, sheet_name='ìš”êµ¬ì‚¬í•­')
                        
                        st.session_state['generated_excel'] = output_buffer.getvalue()
                        st.success("ìš”êµ¬ì‚¬í•­ ëª…ì„¸ì„œ ìƒì„± ì™„ë£Œ! ì•„ë˜ ë²„íŠ¼ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")
                    else:
                        st.warning("ë¶„ì„ ê²°ê³¼, ì¶”ì¶œëœ ìš”êµ¬ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
                except Exception as e:
                    st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    if 'generated_excel' in st.session_state:
        st.download_button(
            label="ëª…ì„¸ì„œ ë‹¤ìš´ë¡œë“œ (.xlsx)", data=st.session_state['generated_excel'],
            file_name="ìš”êµ¬ì‚¬í•­_ëª…ì„¸ì„œ.xlsx", mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            use_container_width=True
        )

# ==================================================================================
# << TAB 2: Side-Effect ì˜ˆì¸¡ê¸° (RAG í¬í•¨) >>
# ==================================================================================
with tab2:
    banner("tab2.png", max_height=400)
    spacer()

    section_heading("GitHub PR URL ì…ë ¥")
    pr_url_tab2 = st.text_input("ğŸ”— PR URL", key="pr_url_input_tab2", placeholder="[https://github.com/owner/repo/pull/123](https://github.com/owner/repo/pull/123)")

    if st.button("PR ì½”ë“œ ë¶ˆëŸ¬ì˜¤ê¸°", key="fetch_pr_tab2", use_container_width=True):
        if not pr_url_tab2:
            st.warning("PR URLì„ ì…ë ¥í•˜ì„¸ìš”.")
        else:
            try:
                owner, repo, pr_no = parse_pr_url(pr_url_tab2)
                with st.spinner("GitHubì—ì„œ ì½”ë“œ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
                    orig, mod = fetch_pr_code_only(owner, repo, pr_no)
                st.session_state.update({"orig_tab2": orig, "mod_tab2": mod})
                st.success("ì½”ë“œ ë¡œë“œ ì™„ë£Œ!")
            except Exception as e:
                st.error(f"GitHub ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}")
    spacer()

    section_heading("ìš”êµ¬ì‚¬í•­ ëª…ì„¸ì„œ ì—…ë¡œë“œ")
    uploaded_req_file_tab2 = st.file_uploader("ìš”êµ¬ì‚¬í•­ ì—‘ì…€(.xlsx) íŒŒì¼ ì—…ë¡œë“œ", type=["xlsx"], key="req_uploader_tab2")
    if uploaded_req_file_tab2 and (req_text_tab2 := read_excel_to_string(uploaded_req_file_tab2)):
        st.session_state["req_text_tab2"] = req_text_tab2
        st.success("ìš”êµ¬ì‚¬í•­ íŒŒì¼ ë¶„ì„ ì™„ë£Œ!")
        with st.expander("ì—…ë¡œë“œëœ ìš”êµ¬ì‚¬í•­ ë³´ê¸°"): st.markdown(req_text_tab2)
    spacer()

    section_heading("ì½”ë“œ í™•ì¸ ë° ìˆ˜ì •")
    st.text_area("ì›ë³¸ ì½”ë“œ", key="orig_tab2", height=180)
    st.text_area("ìˆ˜ì •ëœ ì½”ë“œ", key="mod_tab2", height=180)
    st.markdown("---")

    if st.button("RAG ê¸°ë°˜ ì˜ˆì¸¡ ë° DB ì €ì¥", use_container_width=True, key="predict_btn_tab2"):
        req_content = st.session_state.get("req_text_tab2", "")
        orig_code = st.session_state.get("orig_tab2", "")
        mod_code = st.session_state.get("mod_tab2", "")

        if not pr_url_tab2 or not req_content or not orig_code:
            st.warning("PR URL, ìš”êµ¬ì‚¬í•­, ì½”ë“œë¥¼ ëª¨ë‘ ì¤€ë¹„í•´ì£¼ì„¸ìš”.")
        elif not pinecone_index:
            st.error("Pinecone DBê°€ ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        else:
            with st.spinner("ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ â†’ RAG ì˜ˆì¸¡ â†’ DB ì €ì¥ ì¤‘..."):
                try:
                    owner, repo, pr_no = parse_pr_url(pr_url_tab2)
                    pr_id = f"{owner}/{repo}#{pr_no}"
                    code_diff = make_code_diff(orig_code, mod_code)
                    
                    query_text = f"[ìš”êµ¬ì‚¬í•­]\n{req_content}\n\n[ì½”ë“œ ë³€ê²½]\n{code_diff}"
                    matches = search_similar_prs(query_text)
                    rag_context = "\n".join([f"â–¶ PR: {m.metadata.get('pr_id', '')} (ìœ ì‚¬ë„: {m.score:.2f})\n - ì˜ˆì¸¡ ìš”ì•½: {m.metadata.get('side_effect', '')[:200]}..." for m in matches]) if matches else "ê³¼ê±° ìœ ì‚¬ PR ì—†ìŒ"
                    
                    parser = JsonOutputParser()
                    chain = RAG_PROMPT_TEMPLATE | se_llm | parser
                    response_data = chain.invoke({"rag_context": rag_context, "requirements": req_content, "code_diff": code_diff})
                    
                    st.session_state["final_result_tab2"] = response_data
                    
                    embedding_text = f"[ìš”êµ¬ì‚¬í•­]\n{req_content}\n\n[ì½”ë“œ ë³€ê²½]\n{code_diff}\n\n[ì˜ˆì¸¡ ê²°ê³¼]\n{json.dumps(response_data, ensure_ascii=False, indent=2)}"
                    meta = {"pr_id": pr_id, "title": pr_url_tab2, "desc": req_content[:150], "side_effect": response_data.get('summary', '')[:1000], "url": pr_url_tab2}
                    upsert_to_pinecone(pr_id, embedding_text, meta)
                    
                    st.success("RAG ê¸°ë°˜ ì˜ˆì¸¡ ë° DB ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                except Exception as e:
                    st.error(f"ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    if "final_result_tab2" in st.session_state:
        st.markdown("## RAG ê¸°ë°˜ ì˜ˆì¸¡ ê²°ê³¼ ğŸ“„")
        render_rag_results(st.session_state["final_result_tab2"])

# ==================================================================================
# << TAB 3: Side-Effect ì˜ˆì¸¡ Plus + >>
# ==================================================================================
with tab3:
    banner("tab3.png", max_height=400)
    spacer()

    section_heading("GitHub íŒŒì¼ ê²½ë¡œ ì…ë ¥")
    repo_full = st.text_input("GitHub Repo (ì˜ˆ: xxorud/sunic-user)")
    file_path = st.text_input("íŒŒì¼ ê²½ë¡œ (ì˜ˆ: src/main.py)")
    branch = st.text_input("ë¸Œëœì¹˜ëª… (ì˜ˆ: main)")
    
    if st.button("íŒŒì¼ ì½”ë“œ ë¶ˆëŸ¬ì˜¤ê¸°", key="fetch_file_tab3", use_container_width=True):
        if not repo_full or not file_path:
            st.warning("GitHub ì €ì¥ì†Œì™€ íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        else:
            try:
                owner, repo = repo_full.strip().split("/")
                with st.spinner("GitHubì—ì„œ íŒŒì¼ ì½”ë“œ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
                    file_content = get_github_file(owner, repo, file_path, branch)
                st.session_state["file_content_tab3"] = file_content
                st.success("íŒŒì¼ ì½”ë“œ ë¡œë“œ ì™„ë£Œ!")
            except Exception as e:
                st.error(f"GitHubì—ì„œ íŒŒì¼ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    spacer()

    # ìš”êµ¬ì‚¬í•­ ëª…ì„¸ì„œ ì—…ë¡œë“œ ê¸°ëŠ¥ ì¶”ê°€
    section_heading("ìš”êµ¬ì‚¬í•­ ëª…ì„¸ì„œ ì—…ë¡œë“œ")
    uploaded_req_file_tab3 = st.file_uploader("ìš”êµ¬ì‚¬í•­ ì—‘ì…€(.xlsx) íŒŒì¼ ì—…ë¡œë“œ", type=["xlsx"], key="req_uploader_tab3")
  
    if uploaded_req_file_tab3:
        try:
            # ì—‘ì…€ íŒŒì¼ì„ ì½ì–´ë“¤ì´ê³  ë‚´ìš©ì„ session_stateì— ì €ì¥
            req_text_tab3 = read_excel_to_string(uploaded_req_file_tab3)
            st.session_state["req_text_tab3"] = req_text_tab3
            st.success("ìš”êµ¬ì‚¬í•­ íŒŒì¼ ë¶„ì„ ì™„ë£Œ!")
            with st.expander("ì—…ë¡œë“œëœ ìš”êµ¬ì‚¬í•­ ë³´ê¸°"):
                st.markdown(req_text_tab3)  # ì—‘ì…€ íŒŒì¼ì—ì„œ ì½ì–´ë“¤ì¸ ë‚´ìš©ì„ í™”ë©´ì— ì¶œë ¥
        except Exception as e:
            st.warning(f"ì—‘ì…€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    spacer()

    section_heading("ìˆ˜ì •ëœ ì½”ë“œ ìƒì„± ë° ì˜ˆì¸¡")
    st.text_area("íŒŒì¼ ë‚´ìš©", value=st.session_state.get("file_content_tab3", ""), key="file_content_tab3", height=180)

    req_content = st.session_state.get("req_text_tab3", "")

    # pr_id ìˆ˜ë™ìœ¼ë¡œ ì •ì˜
    pr_id = "manual_pr_id"  # ìˆ˜ë™ìœ¼ë¡œ pr_idë¥¼ ì„¤ì •

    if st.button("ğŸš€ ìˆ˜ì • ì½”ë“œ ìƒì„± ë° Side-Effect ì˜ˆì¸¡", use_container_width=True, key="generate_predict_btn_tab3"):
        file_content = st.session_state.get("file_content_tab3", "")

        if not req_content or not file_content:
            st.warning("ìš”êµ¬ì‚¬í•­ íŒŒì¼ê³¼ íŒŒì¼ ë‚´ìš©ì„ ëª¨ë‘ ì¤€ë¹„í•´ì£¼ì„¸ìš”.")
            
        elif not pinecone_index:
            st.error("Pinecone DBê°€ ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        else:
            st.write(f"ìš”êµ¬ì‚¬í•­: {req_content}")  # ìš”êµ¬ì‚¬í•­ ë‚´ìš© ì¶œë ¥
            modified_code = ""
            with st.spinner("1ë‹¨ê³„: ìš”êµ¬ì‚¬í•­ ê¸°ë°˜ìœ¼ë¡œ ì½”ë“œ ìˆ˜ì • ì¤‘..."):
                try:
                    modified_code = generate_modified_code(req_content, file_content)  # ì´ì œ req_content ì‚¬ìš©
                    st.session_state["modified_code_tab3"] = modified_code
                    st.success("ì½”ë“œ ìˆ˜ì • ì™„ë£Œ!")
                except Exception as e:
                    st.error(f"ì½”ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    st.stop()

            with st.spinner("2ë‹¨ê³„: ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ ë° Side-Effect ì˜ˆì¸¡ ì¤‘..."):
                try:
                    # ì›ë³¸ ì½”ë“œ ë° ìˆ˜ì •ëœ ì½”ë“œë¡œ Side Effect ì˜ˆì¸¡
                    code_diff = make_code_diff(file_content, modified_code)

                    query_text = f"[ìš”êµ¬ì‚¬í•­]\n{req_content}\n\n[ì½”ë“œ ë³€ê²½]\n{code_diff}"
                    matches = search_similar_prs(query_text)
                    rag_context = "\n".join([f"â–¶ PR: {m.metadata.get('pr_id', '')} (ìœ ì‚¬ë„: {m.score:.2f})\n - ì˜ˆì¸¡ ìš”ì•½: {m.metadata.get('side_effect', '')[:200]}..." for m in matches]) if matches else "ê³¼ê±° ìœ ì‚¬ PR ì—†ìŒ"
                    
                    parser = JsonOutputParser()
                    chain = RAG_PROMPT_TEMPLATE | se_llm | parser
                    response_data = chain.invoke({"rag_context": rag_context, "requirements": req_content, "code_diff": code_diff})

                    st.session_state["final_result_tab3"] = response_data
                    
                    embedding_text = f"[ìš”êµ¬ì‚¬í•­]\n{req_content}\n\n[ì½”ë“œ ë³€ê²½]\n{code_diff}\n\n[ì˜ˆì¸¡ ê²°ê³¼]\n{json.dumps(response_data, ensure_ascii=False, indent=2)}"
                    meta = {"pr_id": pr_id, "title": repo_full, "desc": req_content[:150], "side_effect": response_data.get('summary', '')[:1000], "url": repo_full}
                    upsert_to_pinecone(pr_id, embedding_text, meta)

                    st.success("ì˜ˆì¸¡ ë° DB ì €ì¥ ì™„ë£Œ!")
                except Exception as e:
                    st.error(f"ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    if "modified_code_tab3" in st.session_state:
        st.markdown("### AIê°€ ìƒì„±í•œ ìˆ˜ì • ì½”ë“œ")
        st.code(st.session_state["modified_code_tab3"], language="python")

    if "final_result_tab3" in st.session_state:
        st.markdown("## ğŸ¤– RAG ê¸°ë°˜ ì˜ˆì¸¡ ê²°ê³¼")
        render_rag_results(st.session_state["final_result_tab3"])





# --- í•˜ë‹¨ ê³ ì • ë°•ìŠ¤ ---
st.markdown("""
<div class="footer">
    Â© 2025 S-Kape. All rights reserved. | SK mySUNI SUNIC Season 4. #19
</div>
""", unsafe_allow_html=True)
